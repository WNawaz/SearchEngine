{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SearchEngine.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aldolipani/SearchEngine/blob/master/SearchEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o51Jx9eer3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import codecs\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm_notebook\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from google.colab import drive\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "path = None\n",
        "if IN_COLAB:\n",
        "  drive.mount('/content/gdrive')\n",
        "  path = \"./gdrive/My Drive/SearchEngine/\"\n",
        "else:\n",
        "  path = \"./SearchEngine/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhQ-zANIgMd7",
        "colab_type": "text"
      },
      "source": [
        "# Download Test Collection\n",
        "\n",
        "For this example we use a standard test collection known as AdHoc8. This test collection was developed for the Ad Hoc Track of the 8th Text REtrieval Conference (TREC). \n",
        "\n",
        "This test collection consists of: \n",
        "\n",
        "1. A collection of documents;\n",
        "2. A set of topics;\n",
        "3. A set of relevance assessments.\n",
        "\n",
        "To access the collection of documents you need to request a copy through the LDC dataset at this [link](https://catalog.ldc.upenn.edu/LDC93T3A). You can download the set of topics and the relevance assessments from the TREC website at this [link](https://trec.nist.gov/data/test_coll.html).\n",
        "\n",
        "However, if I have given you access to this test collection, you can use the code below to download it. Note that you need to substitute your GitLab username and password in order to download the test collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPTmL_mmRNJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(path + \"test-collection\"):\n",
        "  !rm -fr \"$path\"/test-collection\n",
        "  !git clone https://username:password@gitlab.com/aldolipani/adhoc8.git\n",
        "  !mv adhoc8 \"$path\"/test-collection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T79yDkVnE3ht",
        "colab_type": "text"
      },
      "source": [
        "In case you want to use your own test collection, here I show you the folder structure of the test-collection folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeK3g0bv6gZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"$path\"/test-collection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FALBUb3S6vZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"$path\"/test-collection/Collection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BikBxHIhA-Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"$path\"/test-collection/Topics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njoKJ2thBA2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"$path\"/test-collection/QRels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPAaQ_1NffnY",
        "colab_type": "text"
      },
      "source": [
        "# Parse Collection\n",
        "\n",
        "This code cell parses each file of the Collection folder. From each file it will extract the documents contained therein and create a \n",
        "`collection` dictionary indexed by document id containing the document text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcbIY1yHPqU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "collection = {}\n",
        "\n",
        "if not os.path.exists(path + \"collection.npy\"):\n",
        "  for file_name in tqdm_notebook(list(glob.iglob(path + \"test-collection/Collection/**\", recursive = True))):\n",
        "    if os.path.isfile(file_name):\n",
        "      text = \"<DOCS>\" + codecs.open(file_name, \"r\", \"iso-8859-1\").read() + \"</DOCS>\"\n",
        "      parsed_text = BeautifulSoup(text)\n",
        "      for doc in parsed_text.docs:\n",
        "        id = None\n",
        "        text = []\n",
        "        for field in doc:\n",
        "          if isinstance(field, Tag):\n",
        "            if field.name == \"docno\":\n",
        "              id = field.text.strip()\n",
        "            else:\n",
        "              text.append(field.text.strip())\n",
        "        collection[id] = \"\\n\".join(text)\n",
        "      \n",
        "  np.save(path + \"collection.npy\", collection)\n",
        "else:\n",
        "  print(\"loading\", path + \"collection.npy\")\n",
        "  collection = np.load(path + \"collection.npy\", allow_pickle=True).all()\n",
        "\n",
        "print(str(len(collection)) + \" documents read!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-CqkzVzhO1",
        "colab_type": "text"
      },
      "source": [
        "#Create Direct Index\n",
        "\n",
        "First we define the preprocessing steps performed on every word of the text. \n",
        "\n",
        "Note that this `preprocess` function will be reused for parsing the queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLGYlASaK3_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = nltk.stem.PorterStemmer()\n",
        "preprocess_cache = {}\n",
        "def preprocess(token):\n",
        "  if token not in preprocess_cache:\n",
        "    preprocess_cache[token] = ps.stem(token.lower())\n",
        "  return preprocess_cache[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFD9CAdLK6xK",
        "colab_type": "text"
      },
      "source": [
        "This code cell goes trough each document text in the `collection` dictionary, tokenise it, and then, to each token, apply the `preprocess` function.  The results of this code cell is a `direct_index` dictionary indexed by document id containing the document `bag-of-words`. A bag-of-words is a dictionary indexed by term containing the term frequency of the term in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgrw-rbFKJwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "direct_index = {}\n",
        "\n",
        "if not os.path.exists(path + \"direct_index.npy\"):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for doc in tqdm_notebook(collection):\n",
        "    text = collection[doc]\n",
        "    bag_or_words = {}\n",
        "    for token in nltk.word_tokenize(text):\n",
        "      if not (token in string.punctuation or token in stop_words):\n",
        "        token = preprocess(token)\n",
        "        if token not in bag_or_words:\n",
        "          bag_or_words[token] = 1\n",
        "        else:\n",
        "          bag_or_words[token] += 1\n",
        "    direct_index[doc] = bag_or_words\n",
        "\n",
        "  np.save(path + \"direct_index.npy\", direct_index)\n",
        "else:\n",
        "  print(\"loading\", path + \"direct_index.npy\")\n",
        "  direct_index = np.load(path + \"direct_index.npy\", allow_pickle=True).all()\n",
        "  \n",
        "print(str(len(direct_index)) + \" documents in the direct index!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQlYTbFnLh7s",
        "colab_type": "text"
      },
      "source": [
        "To free space in memory we delete the `collection` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-TbDZrTtOPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del collection # free the memory used by the object collection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUCbGAjpduvt",
        "colab_type": "text"
      },
      "source": [
        "# Create Inverted Index\n",
        "\n",
        "This code cell goes trough each document in the direct index and term in their bag-of-words in order to create an `inverted_index` dictionary. This dictionary indexes by terms `posting` dictionary. Each posting dictionary indexes by  document id the term frequency of that term in this document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnB-qRW9KLwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "inverted_index = {}\n",
        "\n",
        "if not os.path.exists(path + \"inverted_index.npy\"):\n",
        "  for doc in tqdm_notebook(direct_index):\n",
        "    bag_of_words = direct_index[doc]\n",
        "    for term in bag_of_words:\n",
        "      if term not in inverted_index:\n",
        "        inverted_index[term] = {}\n",
        "      inverted_index[term][doc] = bag_of_words[term]\n",
        "    if IN_COLAB:\n",
        "      direct_index[doc] = None\n",
        "  \n",
        "  np.save(path + \"inverted_index.npy\", inverted_index)\n",
        "else:\n",
        "  print(\"loading\", path + \"inverted_index.npy\")\n",
        "  inverted_index = np.load(path + \"inverted_index.npy\", allow_pickle=True).all()\n",
        "    \n",
        "print(str(len(inverted_index)) + \" words in the inverted index!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0kDhM2NL1Ta",
        "colab_type": "text"
      },
      "source": [
        "To free space in memory we delete the `direct_index` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV6IOtbMtQV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del direct_index # free the memory used by the direct index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNmaUu1GVWqi",
        "colab_type": "text"
      },
      "source": [
        "# Read Topics\n",
        "\n",
        "This code cell parses the topic file. This topic file contains three queries types: title, descriptions, and narratives. We will use the title type. This code cell creates a `queries` dictionary indexed by topic id containing a list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJy9T_tzHaRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%time\n",
        "queries = {}\n",
        "\n",
        "reTopic = re.compile(\"<num> Number: (\\d+)\")\n",
        "reTitle = re.compile(\"<title> (.+)\")\n",
        "\n",
        "topic = \"\"\n",
        "title = \"\"\n",
        "for line in codecs.open(path + \"test-collection/Topics/topicsTREC8Adhoc.txt\", \"r\", \"iso-8859-1\").readlines():\n",
        "    mTopic = reTopic.match(line)\n",
        "    mTitle = reTitle.match(line)\n",
        "    \n",
        "    if mTopic:\n",
        "        topic = mTopic.group(1).strip()\n",
        "    elif mTitle:\n",
        "        title = mTitle.group(1).strip()\n",
        "        tokens = nltk.word_tokenize(title)\n",
        "        queries[topic] = []\n",
        "        for token in tokens:\n",
        "            if token not in string.punctuation:\n",
        "                queries[topic].append(preprocess(token))\n",
        "\n",
        "print(str(len(queries)) + \" queries read!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXZ2qlgJVewK",
        "colab_type": "text"
      },
      "source": [
        "# Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5qGQuv_M-zb",
        "colab_type": "text"
      },
      "source": [
        "First we need to compute some collection-wise statistics. These statistics are then used by the scoring functions defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoCekf0mFVJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "lds = {}\n",
        "for term in tqdm_notebook(inverted_index):\n",
        "  for doc in inverted_index[term]:\n",
        "    if doc not in lds:\n",
        "      lds[doc] = 0\n",
        "    lds[doc] += inverted_index[term][doc]\n",
        "\n",
        "D = len(lds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE9L_4meNEkZ",
        "colab_type": "text"
      },
      "source": [
        "We now define the scoring functions BM25 and TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFQgYo5_NIZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lm(tf, df, term, doc):\n",
        "    pass\n",
        "\n",
        "#TF-IDF\n",
        "def tfidf(tf, df, term, doc):\n",
        "    return tf * math.log(D/df)\n",
        "\n",
        "#BM25\n",
        "def bm25(tf, df, term, doc):\n",
        "    ld = lds[doc]\n",
        "    return tf/(tf + 1.2*(0.7 + 0.3*ld/D)) * math.log(D/df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hRRqPD6NTeD",
        "colab_type": "text"
      },
      "source": [
        "We now performe a search for each query using the inverted index. This code cell generates a `runs` dictionary indexed by topic containing a `run`. A run is a dictionary indexed by document id containing the score assigned by the scoring function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpnkvstbVfQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "runs = {}\n",
        "  \n",
        "score = bm25 \n",
        "\n",
        "for topic in queries:    \n",
        "    run = {}\n",
        "    for term in queries[topic]:\n",
        "        if term in inverted_index:\n",
        "            for doc in inverted_index[term]:\n",
        "                tf = inverted_index[term][doc]\n",
        "                df = len(inverted_index[term])\n",
        "                if not doc in run:\n",
        "                    run[doc] = bm25(tf, df, term, doc)\n",
        "                else:\n",
        "                    run[doc] = run[doc] + score(tf, df, term, doc)\n",
        "    \n",
        "    runs[topic] = run\n",
        "\n",
        "print(str(len(runs)) + \" runs retrieved!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIis5hlAVyLS",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqL92TmqOD7U",
        "colab_type": "text"
      },
      "source": [
        "In order to evaluate the `runs` of our search engine we first need to compile the evaluation tool `trec_eval`. This code cell first clones its repository and then compiles it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F0UJzIjHhaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/usnistgov/trec_eval.git\n",
        "!(cd trec_eval && make)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G620WpgTOVYa",
        "colab_type": "text"
      },
      "source": [
        "This code cell writes down the `runs` in trec_eval format. This format consists in a list of lines having the following format:\n",
        "\n",
        "\ttopic \tQ0 \tdocument \trank \tscore \tname\n",
        "\n",
        "Where topic is a number identifying the topic, \n",
        "Q0 is not used, document is the document id, rank is the position at which the document has been retrieved in the run, score is the retrieval score assigned to the document, and name is a string that identifies your search engine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwFmY8JZV4Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm run.txt\n",
        "\n",
        "with open('run.txt', 'a') as the_file:\n",
        "    for topic in queries:\n",
        "        n = 0\n",
        "        run = runs[topic]\n",
        "        for doc in sorted(run, key=run.get, reverse=True):\n",
        "            n = n + 1\n",
        "            if n == 1001:\n",
        "                break\n",
        "            the_file.write(str(topic) + \" Q0 \" + doc + \" \" + str(n) + \" \" + str(run[doc]) + \" run\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3F9fZYaPEap",
        "colab_type": "text"
      },
      "source": [
        "This code will executes `trec_eval` on the generated file using as input also the file defining the set of relevance assessments. \n",
        "\n",
        "Note that a parameter of `trec_eval` is -q. This parameter makes `trec_eval` to compute the evaluation on a per topic basis. The result of this is then piped into `grep` in order to only select MAP as evaluation measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6gTIgPkVlJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./trec_eval/trec_eval -q \"$path\"/test-collection/QRels/qrels.trec8.adhoc.parts1-5 run.txt | grep ^map"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}